{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "450305c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os.path import dirname, abspath\n",
    "path.append(dirname(dirname(abspath(\"__file__\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a13378ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 0.1.43ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Global seed set to 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import statements\n",
    "from transformers import AutoModel, get_cosine_schedule_with_warmup\n",
    "from torch_optimizer import Ranger\n",
    "from torchcrf import CRF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from scripts.utils_bert import *\n",
    "from scripts.metrics import f1score\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from multiprocessing import cpu_count\n",
    "from os import environ\n",
    "from platform import system\n",
    "\n",
    "# set the environment to run tokenizers in parallel\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pl.seed_everything(seed=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675f0f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some important hyperparameters\n",
    "LEARNING_RATE = 5e-4\n",
    "BATCH_SIZE = 10\n",
    "WEIGHT_DECAY = 1e-1\n",
    "EPOCHS = 25\n",
    "N_JOBS = cpu_count() if system() != \"Windows\" else 0\n",
    "\n",
    "BERT_TYPE = \"facebook/bart-base\"\n",
    "MODEL_NAME = f\"{BERT_TYPE}-ner\"\n",
    "TAG2IDX = {'B': 0, 'I': 1, 'O': 2, 'E': 3, 'S': 4, '<': 5, \">\":6, \"$\": 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8793db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_NER(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 bert_type=BERT_TYPE,\n",
    "                 use_scheduler=True,\n",
    "                 num_tags=len(TAG2IDX),\n",
    "                 total_steps=1024,\n",
    "                 train_dataset=None,\n",
    "                 val_dataset=None,\n",
    "                 test_dataset=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_type)\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True)\n",
    "        self.fc = nn.Linear(768, num_tags)\n",
    "        self.use_scheduler = use_scheduler\n",
    "        ## Hyperparameters ##\n",
    "        self.learning_rate = LEARNING_RATE\n",
    "        self.weight_decay = WEIGHT_DECAY\n",
    "        self.total_steps = total_steps\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        ## Datasets ##\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        ## steps ##\n",
    "        if self.use_scheduler: \n",
    "            self.total_steps = len(train_dataset) // self.batch_size\n",
    "\n",
    "\n",
    "    # create the dataloaders\n",
    "    # add shuffle only for train_dataloader\n",
    "    # make sure num_workers is set appropriately and drop_last is set to False\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=True,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        out = self.bert(input_ids, attention_masks).last_hidden_state\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids, masks, lbls = batch\n",
    "        emissions = self(ids, masks)\n",
    "        loss = -self.crf(emissions, lbls, mask=masks)\n",
    "        pred = self.crf.decode(emissions, mask=masks)\n",
    "        r, p, f1 = f1score(lbls, pred, model=\"transformer\")\n",
    "        return loss, r, p, f1\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_recall\", r, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_precision\", p, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_recall\", r, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_precision\", p, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_recall\", r, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_precision\", p, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        ids, masks, _ = batch \n",
    "        return self.crf.decode(self(ids, masks), mask=masks)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):      \n",
    "        optimizer = Ranger(self.parameters(), \n",
    "                           lr=self.learning_rate,\n",
    "                           weight_decay=self.weight_decay)\n",
    "\n",
    "        if self.use_scheduler:\n",
    "            scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer,\n",
    "                                                        num_warmup_steps=1,\n",
    "                                                        num_training_steps=self.total_steps)\n",
    "            lr_scheduler = {\n",
    "                'scheduler': scheduler, \n",
    "                'interval': 'epoch', \n",
    "                'frequency': 1\n",
    "            }\n",
    "            return [optimizer], [lr_scheduler]\n",
    "        else:\n",
    "            return [optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5875c015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da60ee3368c40459137f5013890904e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1caf007402ad43a39d0ac7f341399c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bdcde1fdad4982b3b12ff0b092d08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965c8cc8ef3f43c8a84d842910025d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b08e8e3502f4f80ad4e5430bfe53204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9929471e0f4f5c9a87baceb7c9227a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff04c7054c4b41c184c75482d64f1994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d359296f2e49f790b0c5ced1f8a821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ff5ea5f65e4f439e5970010aca9271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d33899db8e44d08f0540113db866f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d924079455f04e2e8794d66d19d594dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6535c2364aa044609abf25ae65a8023e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the input encoded as numbers along with extended_labels\n",
    "# extended labels have START, END, PAD tokens as well\n",
    "# any word that was split during tokenization would have the same label as the parent label\n",
    "encoded_input, extended_labels = get_encoded_input(\"../../data/train_290818.txt\", \n",
    "                                                   tag2idx=TAG2IDX, \n",
    "                                                   tokenizer_name=BERT_TYPE)\n",
    "\n",
    "L = len(extended_labels)\n",
    "\n",
    "# create a tensor dataset from the input_ids, masks and extended labels\n",
    "# these datasets will help create dataloader for batched execution of data\n",
    "dataset = TensorDataset(torch.LongTensor(encoded_input[\"input_ids\"]),\n",
    "                        torch.BoolTensor(encoded_input[\"attention_mask\"]),\n",
    "                        torch.LongTensor(extended_labels))\n",
    "\n",
    "train_sz, val_sz = L-int(0.1*L), int(0.1*L)\n",
    "# create a random 10% validation split\n",
    "train_dataset, val_dataset = random_split(dataset, (train_sz, val_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6d9705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly read the test data and create the dataset\n",
    "encoded_input, extended_labels = get_encoded_input(\"../../data/test_290818.txt\", \n",
    "                                                   tag2idx=TAG2IDX, \n",
    "                                                   tokenizer_name=BERT_TYPE)\n",
    "\n",
    "test_dataset = TensorDataset(torch.LongTensor(encoded_input[\"input_ids\"]),\n",
    "                             torch.BoolTensor(encoded_input[\"attention_mask\"]),\n",
    "                             torch.LongTensor(extended_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328834d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fc30e8c5c5443ba856265ded3d176d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/532M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# create the model, Trainer object and add necessary callbacks\n",
    "# the trainer saves the weights of model whenever it reaches a local maxima for val_f1score \n",
    "model = BERT_NER(bert_type=BERT_TYPE,\n",
    "                 use_scheduler=True,\n",
    "                 train_dataset=train_dataset,\n",
    "                 val_dataset=val_dataset,\n",
    "                 test_dataset=test_dataset)\n",
    "\n",
    "\n",
    "earlystopping_callback = EarlyStopping(monitor=\"val_f1score\", \n",
    "                                       min_delta=1e-4, \n",
    "                                       patience=5, \n",
    "                                       mode=\"max\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(dirpath=\"../saved_weights\",\n",
    "                                      filename=MODEL_NAME,\n",
    "                                      save_top_k=1, \n",
    "                                      mode=\"max\",\n",
    "                                      monitor=\"val_f1score\",\n",
    "                                      save_weights_only=True)\n",
    "\n",
    "logger = TensorBoardLogger(\"../../tb_logs\", name=MODEL_NAME)\n",
    "\n",
    "# precision=16 runs the model at half-precision\n",
    "# it is faster and consumes lower memory\n",
    "# STRONGLY RECOMMENDED TO RUN THIS CODE ON A GOOD GPU\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     max_epochs=EPOCHS,\n",
    "                     precision=16,\n",
    "                     logger=logger,\n",
    "                     log_every_n_steps=1,\n",
    "                     callbacks=[earlystopping_callback, \n",
    "                                checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe7662",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ../../tb_logs/facebook/bart-base-ner\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type      | Params\n",
      "-----------------------------------\n",
      "0 | bert | BartModel | 139 M \n",
      "1 | crf  | CRF       | 80    \n",
      "2 | fc   | Linear    | 6.2 K \n",
      "-----------------------------------\n",
      "139 M     Trainable params\n",
      "0         Non-trainable params\n",
      "139 M     Total params\n",
      "278.853   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1ee35c55a84feba2102c309c522b65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/varun/.local/lib/python3.10/site-packages/pytorch_ranger/ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1174.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train the model\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c04fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-load the best weights and test the model\n",
    "model.load_state_dict(torch.load(f\"../saved_weights/{MODEL_NAME}.ckpt\")[\"state_dict\"])\n",
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
