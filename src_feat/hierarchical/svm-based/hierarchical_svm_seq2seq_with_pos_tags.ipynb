{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os.path import dirname, abspath\n",
    "path.append(dirname(dirname(dirname(abspath(\"__file__\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for NER model\n",
    "from torchcrf import CRF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scripts.utils import *\n",
    "from scripts.metrics import f1score\n",
    "import pytorch_lightning as pl\n",
    "from multiprocessing import cpu_count\n",
    "from platform import system\n",
    "from pickle import load\n",
    "\n",
    "# for classification model\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pl.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Model (Definition)\n",
    "#### NER model is only used in inference mode by loading the saved_weights\n",
    "#### Hence, all training elements of the model can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAX_LEN = None\n",
    "CELL_TYPE = \"lstm\"\n",
    "MODEL_NAME = f\"seq2seq-uni-{CELL_TYPE}-ner-with-pos-tags\" #remove uni for bidirectional\n",
    "N_JOBS = cpu_count() if system() != \"Windows\" else 0\n",
    "\n",
    "TAG2IDX = {'B': 0, 'I': 1, 'O': 2, 'E': 3, 'S': 4, '<': 5, '>': 6, '$': 7}\n",
    "\n",
    "POS_TAGS2IDX = {'PAD_AUX': 0, 'ADJ': 1, 'ADP': 2, 'ADV': 3, 'AUX': 4, \n",
    "                'CONJ': 5, 'CCONJ': 6, 'DET': 7, 'INTJ': 8, 'NOUN': 9, \n",
    "                'NUM': 10, 'PART': 11, 'PRON': 12, 'PROPN': 13, 'PUNCT': 14, \n",
    "                'SCONJ': 15, 'SYM': 16, 'VERB': 17, 'X': 18, 'SPACE': 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEQ2SEQ_POS_TAGS(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 input_dim1, \n",
    "                 input_dim2,\n",
    "                 cell_type=\"lstm\",\n",
    "                 embed_dim1=128, \n",
    "                 embed_dim2=32,\n",
    "                 dropout=0.5, \n",
    "                 cell_dim1=128,\n",
    "                 cell_dim2=128,\n",
    "                 bidirectional=True, \n",
    "                 num_layers1=3,\n",
    "                 num_layers2=1,\n",
    "                 num_tags=len(TAG2IDX),\n",
    "                 test_dataset=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Embedding(num_embeddings=input_dim1,\n",
    "                                       embedding_dim=embed_dim1,\n",
    "                                       padding_idx=TAG2IDX['$'])\n",
    "\n",
    "        self.embedding2 = nn.Embedding(num_embeddings=input_dim2,\n",
    "                                       embedding_dim=embed_dim2,\n",
    "                                       padding_idx=POS_TAGS2IDX['PAD_AUX'])\n",
    "        \n",
    "        c = (2 if bidirectional else 1)\n",
    "        if cell_type == \"lstm\":\n",
    "            self.cell1 = nn.LSTM(input_size=embed_dim1, \n",
    "                                hidden_size=cell_dim1, \n",
    "                                dropout=dropout,\n",
    "                                batch_first=True,\n",
    "                                num_layers=num_layers1, \n",
    "                                bidirectional=bidirectional)\n",
    "\n",
    "            self.cell2 = nn.LSTM(input_size=embed_dim2, \n",
    "                                hidden_size=cell_dim2, \n",
    "                                dropout=dropout,\n",
    "                                batch_first=True,\n",
    "                                num_layers=num_layers2, \n",
    "                                bidirectional=bidirectional)\n",
    "        else:\n",
    "            self.cell1 = nn.GRU(input_size=embed_dim1, \n",
    "                                hidden_size=cell_dim1, \n",
    "                                dropout=dropout,\n",
    "                                batch_first=True,\n",
    "                                num_layers=num_layers1, \n",
    "                                bidirectional=bidirectional)\n",
    "\n",
    "            self.cell2 = nn.GRU(input_size=embed_dim2, \n",
    "                                hidden_size=cell_dim2, \n",
    "                                dropout=dropout,\n",
    "                                batch_first=True,\n",
    "                                num_layers=num_layers2, \n",
    "                                bidirectional=bidirectional)\n",
    "\n",
    "        self.fc = nn.Linear(c*cell_dim1, num_tags)\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        ## Hyperparameters ##\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        ## Datasets ##\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids1, input_ids2):\n",
    "        out1, _ = self.cell1(self.embedding1(input_ids1))\n",
    "        out2, _ = self.cell2(self.embedding2(input_ids2))\n",
    "        out = out1 + out2\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids1, ids2, masks, lbls = batch\n",
    "        emissions = self(ids1, ids2)\n",
    "        loss = -self.crf(emissions, lbls, mask=masks)\n",
    "        pred = self.crf.decode(emissions, mask=masks)\n",
    "        r, p, f1 = f1score(lbls, pred)\n",
    "        return loss, r, p, f1\n",
    "     \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_recall\", r, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_precision\", p, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use TF-IDF_SVM Model for predicting Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../src_sug/saved_weights/tfidf_vectorizer_ner.pkl\", 'rb') as f:\n",
    "    vectorizer = load(f)\n",
    "\n",
    "with open(\"../../../src_sug/saved_weights/svc_ner.pkl\", 'rb') as f:\n",
    "    svc_classifier = load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_train = pd.read_csv(\"../../../data/train_290818.txt\", \n",
    "                           sep=' ',\n",
    "                           header=None,\n",
    "                           names=['a', 'b', 'c'],\n",
    "                           encoding=\"utf-8\",\n",
    "                           converters={'a': pd.eval, \n",
    "                                       'b': pd.eval})\n",
    "\n",
    "df_ner_train['c'] = df_ner_train['c'].apply(lambda x: 0 if not x else 1)\n",
    "df_ner_train['a'] = df_ner_train['a'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "Xtrain_ner = vectorizer.fit_transform([x for x in df_ner_train['a']])\n",
    "ytrain_ner = df_ner_train['c'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier.fit(Xtrain_ner, ytrain_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_test = pd.read_csv(\"../../../data/test_290818.txt\", \n",
    "                          sep=' ',\n",
    "                          header=None,\n",
    "                          names=['a', 'b', 'c'],\n",
    "                          encoding=\"utf-8\",\n",
    "                          converters={'a': pd.eval, \n",
    "                                      'b': pd.eval})\n",
    "\n",
    "df_ner_test['c'] = df_ner_test['c'].apply(lambda x: 0 if not x else 1)\n",
    "df_ner_test['a'] = df_ner_test['a'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "Xtest_ner = vectorizer.transform([x for x in df_ner_test['a']])\n",
    "ytest_ner = df_ner_test['c'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_ner = svc_classifier.predict(Xtest_ner)\n",
    "print(f\"test f1_score (NER): {f1_score(ytest_ner, ypred_ner, zero_division=0):.4f}\\n\")\n",
    "print(classification_report(ytest_ner, ypred_ner, target_names=['Positive', 'Negative'], digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the sentences which are predicted to have a Suggestion as input to NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_idx = torch.LongTensor([i for (i, y) in enumerate(ypred_ner) if y == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../../data/full_vocab_ner.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = [s.strip() for s in f.readlines()]\n",
    "    VOCAB2IDX = {v:k for (k, v) in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input, pos_tags, masks, extended_labels = get_encoded_input(\"../../../data/test_290818.txt\", \n",
    "                                                                    tag2idx=TAG2IDX,\n",
    "                                                                    vocab2idx=VOCAB2IDX,\n",
    "                                                                    pos_tags2idx=POS_TAGS2IDX,\n",
    "                                                                    return_pos_tags=True,\n",
    "                                                                    maxlen=MAX_LEN)\n",
    "\n",
    "encoded_input = torch.index_select(torch.LongTensor(encoded_input), dim=0, index=sug_idx)\n",
    "pos_tags = torch.index_select(torch.LongTensor(pos_tags), dim=0, index=sug_idx)\n",
    "masks = torch.index_select(torch.BoolTensor(masks), dim=0, index=sug_idx)\n",
    "extended_labels = torch.index_select(torch.LongTensor(extended_labels), dim=0, index=sug_idx)\n",
    "\n",
    "test_dataset = TensorDataset(encoded_input, pos_tags, masks, extended_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEQ2SEQ_POS_TAGS(input_dim1=len(VOCAB2IDX),\n",
    "                         input_dim2=len(POS_TAGS2IDX),\n",
    "                         cell_type=CELL_TYPE,\n",
    "                         bidirectional=False,\n",
    "                         test_dataset=test_dataset)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     precision=16,\n",
    "                     log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"../../saved_weights/{MODEL_NAME}.ckpt\")[\"state_dict\"])\n",
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
