{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "from os.path import dirname, abspath\n",
    "path.append(dirname(dirname(dirname(abspath(\"__file__\")))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for NER model\n",
    "from transformers import AutoModel\n",
    "from torchcrf import CRF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from scripts.utils_bert import *\n",
    "from scripts.metrics import f1score\n",
    "import pytorch_lightning as pl\n",
    "from multiprocessing import cpu_count\n",
    "from platform import system\n",
    "from os import environ\n",
    "from itertools import chain\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# set the environment to run tokenizers in parallel\n",
    "environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "pl.seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Model (Definition)\n",
    "#### NER model is only used in inference mode by loading the saved_weights\n",
    "#### Hence, all training elements of the model can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some important hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "N_JOBS = cpu_count() if system() != \"Windows\" else 0\n",
    "\n",
    "BERT_TYPE = \"roberta-base\"\n",
    "TAG2IDX = {'B': 0, 'I': 1, 'O': 2, 'E': 3, 'S': 4, '<': 5, \">\":6, \"$\": 7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_NER(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 bert_type=BERT_TYPE, \n",
    "                 num_tags=len(TAG2IDX),\n",
    "                 test_dataset=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_type)\n",
    "        self.crf = CRF(num_tags=num_tags, batch_first=True)\n",
    "        self.fc = nn.Linear(768, num_tags)\n",
    "        ## Hyperparameters ##\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        ## Datasets ##\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        out = self.bert(input_ids, attention_masks).last_hidden_state\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids, masks, lbls = batch\n",
    "        emissions = self(ids, masks)\n",
    "        loss = -self.crf(emissions, lbls, mask=masks)\n",
    "        pred = self.crf.decode(emissions, mask=masks)\n",
    "        r, p, f1 = f1score(lbls, pred, model=\"transformer\")\n",
    "        return loss, r, p, f1\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, r, p, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_recall\", r, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_precision\", p, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa-Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_SUG(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 dropout=0.5,\n",
    "                 bert_type=BERT_TYPE,\n",
    "                 test_dataset=None):\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_type)\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(768)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        ## Hyperparameters ##\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        ## Datasets ##\n",
    "        self.test_dataset = test_dataset\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, \n",
    "                          batch_size=self.batch_size,\n",
    "                          num_workers=N_JOBS,\n",
    "                          shuffle=False,\n",
    "                          drop_last=False)\n",
    "    \n",
    "    \n",
    "    def _f1score(self, logits, lbls):\n",
    "        lbls = torch.flatten(lbls)\n",
    "        preds = torch.flatten(torch.round(torch.sigmoid(logits)))\n",
    "        return f1_score(lbls.tolist(), preds.tolist(), zero_division=0)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_masks):\n",
    "        out = self.bert(input_ids, attention_masks).pooler_output\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    def _shared_evaluation_step(self, batch, batch_idx):\n",
    "        ids, masks, lbls = batch\n",
    "        logits = self(ids, masks)\n",
    "        loss = self.loss_fn(logits, lbls.float())\n",
    "        f1 = self._f1score(logits, lbls)\n",
    "        return loss, f1\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, f1 = self._shared_evaluation_step(batch, batch_idx)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"test_f1score\", f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        ids, masks, _ = batch\n",
    "        logits = self(ids, masks)\n",
    "        preds = torch.flatten(torch.round(torch.sigmoid(logits)))\n",
    "        return preds.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use RoBERTa-Classification Model for predicting Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly read the test data and create the dataset\n",
    "encoded_input, targets = get_encoded_input(\"../../../data/test_290818.txt\", \n",
    "                                           tag2idx=TAG2IDX, \n",
    "                                           tokenizer_name=BERT_TYPE,\n",
    "                                           return_classification_targets=True)\n",
    "\n",
    "test_dataset = TensorDataset(torch.LongTensor(encoded_input[\"input_ids\"]),\n",
    "                             torch.BoolTensor(encoded_input[\"attention_mask\"]),\n",
    "                             torch.LongTensor(targets).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_SUG(bert_type=BERT_TYPE,\n",
    "                 test_dataset=test_dataset)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     precision=16,\n",
    "                     log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"../../../src_sug/saved_weights/{BERT_TYPE}-sug-on-NER-dataset.ckpt\")[\"state_dict\"])\n",
    "trainer.test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(chain(*trainer.predict(model)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the sentences which are predicted to have a Suggestion as input to NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug_idx = torch.LongTensor([i for (i, y) in enumerate(preds) if y == 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarly read the test data and create the dataset\n",
    "encoded_input, extended_labels = get_encoded_input(\"../../../data/test_290818.txt\", \n",
    "                                                   tag2idx=TAG2IDX, \n",
    "                                                   tokenizer_name=BERT_TYPE)\n",
    "\n",
    "test_dataset = TensorDataset(torch.index_select(torch.LongTensor(encoded_input[\"input_ids\"]), dim=0, index=sug_idx),\n",
    "                             torch.index_select(torch.BoolTensor(encoded_input[\"attention_mask\"]), dim=0, index=sug_idx),\n",
    "                             torch.index_select(torch.LongTensor(extended_labels), dim=0, index=sug_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT_NER(bert_type=BERT_TYPE,\n",
    "                 test_dataset=test_dataset)\n",
    "\n",
    "trainer = pl.Trainer(accelerator=\"gpu\",\n",
    "                     precision=16,\n",
    "                     log_every_n_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f\"../../saved_weights/{BERT_TYPE}-ner.ckpt\")[\"state_dict\"])\n",
    "trainer.test(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
